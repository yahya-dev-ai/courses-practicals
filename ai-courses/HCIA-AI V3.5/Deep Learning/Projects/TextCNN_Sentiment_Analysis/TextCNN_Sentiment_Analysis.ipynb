{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b84f920",
   "metadata": {},
   "source": [
    "# üìù TextCNN Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722fc786",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>üìã About This Project & Objectives</strong></summary>\n",
    "\n",
    "This exercise implements the TextCNN sentiment analysis, a classic case in the deep learning field. This project demonstrates advanced natural language processing techniques using convolutional neural networks for text classification.\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "- üìù Master text preprocessing and tokenization techniques for sentiment analysis\n",
    "- üèóÔ∏è Build and configure TextCNN architecture with multiple filter sizes\n",
    "- ‚öôÔ∏è Implement text classification training strategies with embeddings\n",
    "\n",
    "**Project Workflow:**\n",
    "- ‚ö´ Download the required dataset (rt-polarity dataset)\n",
    "- ‚ö´ Define data preprocessing function and generate data for training and validation\n",
    "- ‚ö´ Define the TextCNN model structure build, training, validation, offline model loading, and online inference functions\n",
    "- ‚ö´ Define various parameters required for training, such as optimizer, loss function, checkpoint, and time monitor\n",
    "- ‚ö´ Load the dataset and perform training\n",
    "- ‚ö´ After training completion, use the test set for validation\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b766e1b8",
   "metadata": {},
   "source": [
    "## üìö Imports and Environment Setup\n",
    "Setting up the required libraries and environment for TextCNN sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01c84aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "#  Basic Python libraries\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import codecs\n",
    "from pathlib import Path\n",
    "\n",
    "# MindSpore framework\n",
    "import mindspore\n",
    "import mindspore.dataset as ds\n",
    "import mindspore.nn as nn\n",
    "from mindspore import Tensor\n",
    "from mindspore import context\n",
    "\n",
    "# Training components\n",
    "from mindspore.train.model import Model\n",
    "from mindspore.nn.metrics import Accuracy\n",
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor, TimeMonitor\n",
    "from mindspore.ops import operations as ops\n",
    "\n",
    "# Configuration utilities\n",
    "from easydict import EasyDict as edict # Used to create a dictionary with attribute-style access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2821d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters for TextCNN sentiment analysis\n",
    "cfg = edict({\n",
    "    # Project configuration\n",
    "    'name': 'movie review',\n",
    "    'pre_trained': False,\n",
    "    'num_classes': 2,\n",
    "    \n",
    "    # Training parameters\n",
    "    'batch_size': 64,\n",
    "    'epoch_size': 4,\n",
    "    'weight_decay': 3e-5,\n",
    "    \n",
    "    # Data configuration\n",
    "    'data_path': './data/',\n",
    "    'word_len': 51,\n",
    "    'vec_length': 40,\n",
    "    \n",
    "    # Device and checkpoint configuration\n",
    "    'device_target': 'CPU',\n",
    "    'device_id': 0,\n",
    "    'keep_checkpoint_max': 1,\n",
    "    'checkpoint_path': 'ckpt/train_textcnn-4_596.ckpt',\n",
    "})\n",
    "\n",
    "# Set MindSpore execution context\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=cfg.device_target, \n",
    "                    device_id=cfg.device_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f198b08c",
   "metadata": {},
   "source": [
    "## üìä Dataset Download and Preprocessing\n",
    "Processing the rt-polarity dataset for sentiment analysis with TextCNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c112644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative reivews:\n",
      "[0]:simplistic , silly and tedious . \n",
      "\n",
      "[1]:it's so laddish and juvenile , only teenage boys could possibly find it funny . \n",
      "\n",
      "[2]:exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable . \n",
      "\n",
      "[3]:[garbus] discards the potential for pathological study , exhuming instead , the skewed melodrama of the circumstantial situation . \n",
      "\n",
      "[4]:a visually flashy but narratively opaque and emotionally vapid exercise in style and mystification . \n",
      "\n",
      "Positive reivews:\n",
      "[0]:the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \n",
      "\n",
      "[1]:the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth . \n",
      "\n",
      "[2]:effective but too-tepid biopic\n",
      "\n",
      "[3]:if you sometimes like to go to the movies to have fun , wasabi is a good place to start . \n",
      "\n",
      "[4]:emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read data.\n",
    "# Read negative reviews\n",
    "with open(\"./data/rt-polarity.neg\", 'r', encoding='utf-8') as f:\n",
    "    print(\"Negative reivews:\")\n",
    "    for i in range(5):\n",
    "        print(\"[{0}]:{1}\".format(i,f.readline()))\n",
    "\n",
    "# Read positive reviews\n",
    "with open(\"./data/rt-polarity.pos\", 'r', encoding='utf-8') as f:\n",
    "    print(\"Positive reivews:\")\n",
    "    for i in range(5):\n",
    "        print(\"[{0}]:{1}\".format(i,f.readline()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "284a08b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data generation class.\n",
    "class Generator():\n",
    "    def __init__(self, input_list):\n",
    "        self.input_list=input_list\n",
    "    \n",
    "    def __getitem__(self,item):\n",
    "        return (np.array(self.input_list[item][0],dtype=np.int32),\n",
    "                np.array(self.input_list[item][1],dtype=np.int32))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30b0b056",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieReview:\n",
    "    '''\n",
    "    Movie review dataset\n",
    "    '''\n",
    "    def __init__(self, root_dir, maxlen, split):\n",
    "        '''\n",
    "        input:\n",
    "        root_dir: movie review data directory\n",
    "        maxlen: maximum length of a sentence\n",
    "        split: the training/validation ratio in the dataset\n",
    "        '''\n",
    "        self.path = root_dir\n",
    "        self.feelMap = {\n",
    "            'neg':0,\n",
    "            'pos':1\n",
    "        }\n",
    "        self.files = []\n",
    "        self.doConvert = False\n",
    "        \n",
    "        mypath = Path(self.path)\n",
    "        if not mypath.exists() or not mypath.is_dir():\n",
    "            print(\"please check the root_dir!\")\n",
    "            raise ValueError\n",
    "        \n",
    "        # Find the files in the data directory.\n",
    "        # Check whether there are two files: .neg and .pos.\n",
    "        for root, dirs, files in os.walk(self.path):\n",
    "            for file in files:\n",
    "                if file.endswith('.neg') or file.endswith('.pos'):\n",
    "                    self.files.append(os.path.join(root, file))\n",
    "        \n",
    "        # Read data.\n",
    "        self.word_num = 0\n",
    "        self.maxlen = 0\n",
    "        self.minlen = float(\"inf\")\n",
    "        self.maxlen = float(\"-inf\")\n",
    "        self.Pos = []\n",
    "        self.Neg = []\n",
    "        \n",
    "        for filename in self.files:\n",
    "            self.read_data(filename)\n",
    "        \n",
    "        self.text2vec(maxlen=maxlen)\n",
    "        self.split_dataset(split=split)\n",
    "\n",
    "    def read_data(self, filePath):\n",
    "        \"\"\"\n",
    "        Data reading and preprocessing method\n",
    "        \"\"\"\n",
    "        with open(filePath,'r') as f:\n",
    "            for sentence in f.readlines():\n",
    "                # Text cleaning - remove punctuation and special characters\n",
    "                sentence = sentence.replace('\\n','')\\\n",
    "                    .replace('\"','')\\\n",
    "                    .replace('\\'','')\\\n",
    "                    .replace('.','')\\\n",
    "                    .replace(',','')\\\n",
    "                    .replace('[','')\\\n",
    "                    .replace(']','')\\\n",
    "                    .replace('(','')\\\n",
    "                    .replace(')','')\\\n",
    "                    .replace(':','')\\\n",
    "                    .replace('--','')\\\n",
    "                    .replace('-',' ')\\\n",
    "                    .replace('\\\\','')\\\n",
    "                    .replace('0','')\\\n",
    "                    .replace('1','')\\\n",
    "                    .replace('2','')\\\n",
    "                    .replace('3','')\\\n",
    "                    .replace('4','')\\\n",
    "                    .replace('5','')\\\n",
    "                    .replace('6','')\\\n",
    "                    .replace('7','')\\\n",
    "                    .replace('8','')\\\n",
    "                    .replace('9','')\\\n",
    "                    .replace('`','')\\\n",
    "                    .replace('=','')\\\n",
    "                    .replace('$','')\\\n",
    "                    .replace('/','')\\\n",
    "                    .replace('*','')\\\n",
    "                    .replace(';','')\\\n",
    "                    .replace('<b>','')\\\n",
    "                    .replace('%','')\n",
    "                \n",
    "                # Tokenization and filtering\n",
    "                sentence = sentence.split(' ')\n",
    "                sentence = list(filter(lambda x: x, sentence))\n",
    "                \n",
    "                # Statistics and classification\n",
    "                if sentence:\n",
    "                    self.word_num += len(sentence)\n",
    "                    self.maxlen = self.maxlen if self.maxlen >= len(sentence) else len(sentence)\n",
    "                    self.minlen = self.minlen if self.minlen <= len(sentence) else len(sentence)\n",
    "                    \n",
    "                    if 'pos' in filePath:\n",
    "                        self.Pos.append([sentence,self.feelMap['pos']])\n",
    "                    else:\n",
    "                        self.Neg.append([sentence,self.feelMap['neg']])\n",
    "\n",
    "    def text2vec(self, maxlen):\n",
    "        \"\"\"\n",
    "        Text vectorization method - Convert sentences into vectors.\n",
    "        \"\"\"\n",
    "        # Create vocabulary dictionary\n",
    "        # Vocab = {word : index}\n",
    "        self.Vocab = {'None': 0}\n",
    "        vocab_index = 1\n",
    "        \n",
    "        # Build vocabulary from all sentences\n",
    "        for SentenceLabel in self.Pos+self.Neg:\n",
    "            for word in SentenceLabel[0]:\n",
    "                if word not in self.Vocab.keys():\n",
    "                    self.Vocab[word] = vocab_index\n",
    "                    vocab_index += 1\n",
    "        \n",
    "        # Convert sentences to vectors\n",
    "        for SentenceLabel in self.Pos+self.Neg:\n",
    "            vector = [0]*maxlen\n",
    "            for index, word in enumerate(SentenceLabel[0]):\n",
    "                if index >= maxlen:\n",
    "                    break\n",
    "                if word not in self.Vocab.keys():\n",
    "                    vector[index] = 0  # self.Vocab['None']\n",
    "                else:\n",
    "                    vector[index] = self.Vocab[word]\n",
    "            SentenceLabel[0] = vector\n",
    "        \n",
    "        # Mark conversion as complete\n",
    "        self.doConvert = True\n",
    "\n",
    "    def split_dataset(self, split):\n",
    "        \"\"\"\n",
    "        Dataset splitting method - Divide the dataset into a training set and a test set.\n",
    "        \"\"\"\n",
    "        # Calculate split sizes\n",
    "        trunk_pos_size = math.ceil((1-split)*len(self.Pos))\n",
    "        trunk_neg_size = math.ceil((1-split)*len(self.Neg))\n",
    "        trunk_num = int(1/(1-split))\n",
    "        \n",
    "        # Create temporary lists for splitting\n",
    "        pos_temp=list()\n",
    "        neg_temp=list()\n",
    "        \n",
    "        # Split data into chunks\n",
    "        for index in range(trunk_num):\n",
    "            pos_temp.append(self.Pos[index*trunk_pos_size:(index+1)*trunk_pos_size])\n",
    "            neg_temp.append(self.Neg[index*trunk_neg_size:(index+1)*trunk_neg_size])\n",
    "        \n",
    "        # Create test and train sets\n",
    "        self.test = pos_temp.pop(2)+neg_temp.pop(2)\n",
    "        self.train = [i for item in pos_temp+neg_temp for i in item]\n",
    "        random.shuffle(self.train)\n",
    "\n",
    "    def get_dict_len(self):\n",
    "        \"\"\"\n",
    "        Vocabulary dictionary utility method - Obtain the length of a dictionary consisting of characters in a dataset.\n",
    "        \"\"\"\n",
    "        # Check if text vectorization is complete\n",
    "        if self.doConvert:\n",
    "            return len(self.Vocab)\n",
    "        else:\n",
    "            print(\"Haven't finished Text2Vec\")\n",
    "            return -1\n",
    "\n",
    "    def create_train_dataset(self, epoch_size, batch_size):\n",
    "        \"\"\"\n",
    "        Training dataset creation method\n",
    "        \"\"\"\n",
    "        # Create generator dataset for training\n",
    "        dataset = ds.GeneratorDataset(\n",
    "            source=Generator(input_list=self.train), \n",
    "            column_names=[\"data\",\"label\"], \n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        # Apply batching and repetition for training\n",
    "        dataset=dataset.batch(batch_size=batch_size,drop_remainder=True)\n",
    "        dataset=dataset.repeat(epoch_size)\n",
    "        return dataset\n",
    "\n",
    "    def create_test_dataset(self, batch_size):\n",
    "        \"\"\"\n",
    "        Test dataset creation method\n",
    "        \"\"\"\n",
    "        # Create generator dataset for testing\n",
    "        dataset = ds.GeneratorDataset(\n",
    "            source=Generator(input_list=self.test), \n",
    "            column_names=[\"data\",\"label\"], \n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        # Apply batching for testing\n",
    "        dataset=dataset.batch(batch_size=batch_size,drop_remainder=True)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "322e624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MovieReview instance and prepare training dataset\n",
    "instance = MovieReview(root_dir=cfg.data_path, maxlen=cfg.word_len, split=0.9)\n",
    "dataset = instance.create_train_dataset(batch_size=cfg.batch_size, epoch_size=cfg.epoch_size)\n",
    "batch_num = dataset.get_dataset_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3988ee9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 18849\n",
      "{'data': Tensor(shape=[64, 51], dtype=Int32, value=\n",
      "[[   16,   729, 11161 ...     0,     0,     0],\n",
      " [  365, 14030,   416 ...     0,     0,     0],\n",
      " [10430,    11, 10431 ...     0,     0,     0],\n",
      " ...\n",
      " [ 1771,   129,    69 ...     0,     0,     0],\n",
      " [  108,  6943,    20 ...     0,     0,     0],\n",
      " [   16, 13816,   248 ...     0,     0,     0]]), 'label': Tensor(shape=[64], dtype=Int32, value= [1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, \n",
      " 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, \n",
      " 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0])}\n",
      "[  365 14030   416   146  5334    11 12482   248     1  3734    11  9241\n",
      "    85    33    76  2723   974  1284   416     1  1391    33  8627     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0]\n",
      "[  365 14030   416   146  5334    11 12482   248     1  3734    11  9241\n",
      "    85    33    76  2723   974  1284   416     1  1391    33  8627     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "# Display the data processing results\n",
    "vocab_size = instance.get_dict_len()\n",
    "print(\"vocab_size: {0}\".format(vocab_size))\n",
    "\n",
    "# Show sample data from the dataset\n",
    "item = dataset.create_dict_iterator()\n",
    "for i, data in enumerate(item):\n",
    "    if i < 1:\n",
    "        print(data)\n",
    "        print(data['data'][1])\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c45cfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure learning rate schedule with warm-up, normal, and shrink phases\n",
    "learning_rate = []\n",
    "\n",
    "# Warm-up phase: gradually increase learning rate\n",
    "warm_up = [1e-3 / math.floor(cfg.epoch_size / 5) * (i + 1) for _ in range(batch_num) \n",
    "           for i in range(math.floor(cfg.epoch_size / 5))]\n",
    "\n",
    "# Shrink phase: gradually decrease learning rate\n",
    "shrink = [1e-3 / (16 * (i + 1)) for _ in range(batch_num) \n",
    "          for i in range(math.floor(cfg.epoch_size * 3 / 5))]\n",
    "\n",
    "# Normal phase: constant learning rate\n",
    "normal_run = [1e-3 for _ in range(batch_num) for i in \n",
    "              range(cfg.epoch_size - math.floor(cfg.epoch_size / 5) \n",
    "                    - math.floor(cfg.epoch_size * 2 / 5))]\n",
    "\n",
    "# Combine all phases\n",
    "learning_rate = learning_rate + warm_up + normal_run + shrink"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1bafba",
   "metadata": {},
   "source": [
    "## üèóÔ∏è TextCNN Architecture\n",
    "Building the TextCNN network with multiple convolutional filters for text sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af77e286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextCNN Model Definition\n",
    "# The following cells define the TextCNN architecture with helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "441a5479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for weight initialization\n",
    "def _weight_variable(shape, factor=0.01):\n",
    "    \"\"\"Initialize weights with random values\"\"\"\n",
    "    init_value = np.random.randn(*shape).astype(np.float32) * factor\n",
    "    return Tensor(init_value)\n",
    "\n",
    "\n",
    "# Helper function to create convolutional layers\n",
    "def make_conv_layer(kernel_size):\n",
    "    \"\"\"Create a convolutional layer with specified kernel size\"\"\"\n",
    "    weight_shape = (96, 1, *kernel_size)\n",
    "    weight = _weight_variable(weight_shape)\n",
    "    return nn.Conv2d(in_channels=1, out_channels=96, kernel_size=kernel_size, padding=1,\n",
    "                     pad_mode=\"pad\", weight_init=weight, has_bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fd863f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextCNN model class definition\n",
    "class TextCNN(nn.Cell):\n",
    "    def __init__(self, vocab_len, word_len, num_classes, vec_length):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.vec_length = vec_length\n",
    "        self.word_len = word_len\n",
    "        self.num_classes = num_classes\n",
    "        self.unsqueeze = ops.ExpandDims()\n",
    "        self.embedding = nn.Embedding(vocab_len, self.vec_length, embedding_table='normal')\n",
    "        self.slice = ops.Slice()\n",
    "        self.layer1 = self.make_layer(kernel_height=3)\n",
    "        self.layer2 = self.make_layer(kernel_height=4)\n",
    "        self.layer3 = self.make_layer(kernel_height=5)\n",
    "        self.concat = ops.Concat(1)\n",
    "        self.fc = nn.Dense(96*3, self.num_classes)\n",
    "        self.drop = nn.Dropout(keep_prob=0.5)\n",
    "        self.print = ops.Print()\n",
    "        self.reducemean = ops.ReduceMax(keep_dims=False)\n",
    "        \n",
    "    def make_layer(self, kernel_height):\n",
    "        return nn.SequentialCell(\n",
    "            [\n",
    "                make_conv_layer((kernel_height,self.vec_length)),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=(self.word_len-kernel_height+1,1)),\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def construct(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the TextCNN model.\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): Input tensor with shape (batch_size, word_len)\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: Output logits for classification\n",
    "        \"\"\"\n",
    "        # Convert input to embeddings\n",
    "        x = self.embedding(x)  # Shape: (batch_size, word_len, vec_length)\n",
    "        \n",
    "        # Add channel dimension for convolution\n",
    "        x = self.unsqueeze(x, 1)  # Shape: (batch_size, 1, word_len, vec_length)\n",
    "        \n",
    "        # Apply different convolutional layers with different kernel sizes\n",
    "        x1 = self.layer1(x)  # Shape: (batch_size, 96, 1, 1)\n",
    "        x2 = self.layer2(x)  # Shape: (batch_size, 96, 1, 1)\n",
    "        x3 = self.layer3(x)  # Shape: (batch_size, 96, 1, 1)\n",
    "        \n",
    "        # Flatten the feature maps\n",
    "        x1 = self.reducemean(x1, (2, 3))  # Shape: (batch_size, 96)\n",
    "        x2 = self.reducemean(x2, (2, 3))  # Shape: (batch_size, 96)\n",
    "        x3 = self.reducemean(x3, (2, 3))  # Shape: (batch_size, 96)\n",
    "        \n",
    "        # Concatenate features from all convolutional layers\n",
    "        x = self.concat((x1, x2, x3))  # Shape: (batch_size, 288)\n",
    "        \n",
    "        # Apply dropout for regularization\n",
    "        x = self.drop(x)\n",
    "        \n",
    "        # Final classification layer\n",
    "        x = self.fc(x)  # Shape: (batch_size, num_classes)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bfd5e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(11479:130224391405696,MainProcess):2025-09-12-15:52:38.874.21 [mindspore/nn/layer/basic.py:173] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(11479:130224391405696,MainProcess):2025-09-12-15:52:38.929.91 [mindspore/nn/layer/basic.py:199] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(11479:130224391405696,MainProcess):2025-09-12-15:52:38.929.91 [mindspore/nn/layer/basic.py:199] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextCNN Model Architecture:\n",
      "TextCNN<\n",
      "  (embedding): Embedding<vocab_size=18849, embedding_size=40, use_one_hot=False, embedding_table=Parameter (name=embedding.embedding_table, shape=(18849, 40), dtype=Float32, requires_grad=True), dtype=Float32, padding_idx=None>\n",
      "  (layer1): SequentialCell<\n",
      "    (0): Conv2d<input_channels=1, output_channels=96, kernel_size=(3, 40), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=[[[[-0.01620523  0.00430576  0.00627976 ...  0.01322129 -0.00171179\n",
      "         0.00499074]\n",
      "       [ 0.00504958 -0.00449969 -0.01056014 ... -0.00311921 -0.01817112\n",
      "         0.01028888]\n",
      "       [-0.00263585 -0.01258848 -0.01530088 ...  0.00794003  0.02201227\n",
      "         0.00944341]]]\n",
      "    \n",
      "    \n",
      "     [[[ 0.00044081 -0.00835892 -0.00865513 ...  0.01416665 -0.0018302\n",
      "        -0.0018223 ]\n",
      "       [ 0.0104546  -0.00342864 -0.00826981 ... -0.01911463 -0.0002123\n",
      "        -0.00121759]\n",
      "       [-0.02338507 -0.00255891  0.00063968 ...  0.01364763 -0.00033362\n",
      "        -0.0089794 ]]]\n",
      "    \n",
      "    \n",
      "     [[[ 0.00527862  0.01369985 -0.01038782 ...  0.01493175 -0.01160424\n",
      "        -0.0063308 ]\n",
      "       [-0.01451484  0.00767187  0.00822204 ...  0.00927585  0.0006899\n",
      "        -0.0132302 ]\n",
      "       [-0.03264463  0.00351652  0.01514929 ... -0.02618821  0.01385001\n",
      "         0.00916486]]]\n",
      "    \n",
      "    \n",
      "     ...\n",
      "    \n",
      "    \n",
      "     [[[-0.00502654 -0.00512953  0.01009901 ... -0.00662535  0.01213739\n",
      "        -0.00642108]\n",
      "       [-0.01159541  0.00320341 -0.00883458 ... -0.00172748 -0.01207086\n",
      "        -0.00665139]\n",
      "       [ 0.00048065  0.00083181 -0.00346694 ... -0.00314927 -0.00049263\n",
      "        -0.0047576 ]]]\n",
      "    \n",
      "    \n",
      "     [[[-0.00641924 -0.00321205 -0.00190486 ... -0.00129448  0.01549008\n",
      "         0.00725813]\n",
      "       [-0.01507173  0.00117745  0.00056415 ... -0.01442447  0.02965083\n",
      "        -0.00754951]\n",
      "       [-0.00400343 -0.00047223  0.0071402  ... -0.00266416  0.00448819\n",
      "        -0.00811575]]]\n",
      "    \n",
      "    \n",
      "     [[[-0.01219614 -0.00547158  0.01281889 ...  0.00857876  0.00209595\n",
      "         0.00412371]\n",
      "       [-0.00864658 -0.00512275  0.00861374 ... -0.01297928 -0.00456934\n",
      "         0.01732783]\n",
      "       [-0.00704225  0.00620101 -0.00245768 ...  0.0022698  -0.01143856\n",
      "        -0.00577799]]]], bias_init=<mindspore.common.initializer.Uniform object at 0x766fe55bb050>, format=NCHW>\n",
      "    (1): ReLU<>\n",
      "    (2): MaxPool2d<kernel_size=(49, 1), stride=1, pad_mode=VALID>\n",
      "    >\n",
      "  (layer2): SequentialCell<\n",
      "    (0): Conv2d<input_channels=1, output_channels=96, kernel_size=(4, 40), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=[[[[-1.48001108e-02 -5.75225940e-03  1.58283813e-03 ...  2.89269467e-03\n",
      "        -2.36435458e-02  2.93269660e-03]\n",
      "       [ 8.41313566e-04  1.32559603e-02 -1.02417869e-02 ...  8.06149619e-04\n",
      "         2.00810134e-02  5.98840602e-03]\n",
      "       [ 1.79031994e-02 -1.86499394e-03 -1.26394853e-02 ... -5.71493665e-03\n",
      "        -1.52909383e-02  1.35502052e-02]\n",
      "       [ 5.63783571e-03 -3.54513650e-05  2.09091185e-03 ...  1.08333407e-02\n",
      "         6.74522202e-03 -5.59006911e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[ 1.78919937e-02 -8.75505712e-03 -8.07446148e-03 ... -2.77133513e-04\n",
      "         2.93832854e-03  9.83119942e-03]\n",
      "       [-8.04458465e-03 -1.29692419e-03  9.89044551e-03 ... -6.92679500e-03\n",
      "         5.90377161e-03 -1.22447861e-02]\n",
      "       [ 5.29786758e-03 -1.17717264e-02 -1.47798173e-02 ... -1.23620881e-02\n",
      "        -2.13977462e-03 -1.24852806e-02]\n",
      "       [ 8.21358804e-03  1.11584496e-02 -4.53113113e-03 ... -1.54781351e-02\n",
      "        -1.42602716e-02  1.49285505e-02]]]\n",
      "    \n",
      "    \n",
      "     [[[-7.69408047e-03 -4.85760067e-03 -6.82014460e-03 ...  2.17296346e-03\n",
      "         1.47567606e-02 -5.79897454e-03]\n",
      "       [ 1.17450568e-03  2.43168045e-02 -2.02733150e-04 ... -1.96345150e-03\n",
      "        -2.13594199e-03 -8.67733359e-03]\n",
      "       [-3.68337583e-04  8.20917636e-03 -4.46046563e-03 ... -1.33349393e-02\n",
      "         5.54916309e-03  6.94663683e-03]\n",
      "       [-1.31968729e-04 -1.50392521e-02  5.31660253e-03 ... -1.41284717e-02\n",
      "         1.16664963e-02 -2.57680682e-03]]]\n",
      "    \n",
      "    \n",
      "     ...\n",
      "    \n",
      "    \n",
      "     [[[ 6.55353907e-03  1.49946781e-02  1.13486201e-02 ...  1.60896464e-03\n",
      "        -1.56532284e-02 -1.99181717e-02]\n",
      "       [ 1.15243793e-02 -5.29480260e-03 -9.42681450e-03 ...  5.78253949e-03\n",
      "        -8.77020764e-04 -7.14225322e-03]\n",
      "       [ 7.36971339e-03  1.58941327e-03  8.55057873e-03 ...  8.78903549e-03\n",
      "        -7.84388371e-03  7.14297034e-03]\n",
      "       [ 3.90494824e-03  5.58849145e-03 -1.70177605e-03 ...  1.38559155e-02\n",
      "        -3.84034263e-03 -5.75218908e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[-1.64226629e-02 -1.00319507e-02 -1.58181321e-02 ... -5.98597806e-03\n",
      "        -4.43641189e-03 -1.69513712e-03]\n",
      "       [-8.01782589e-04  1.78458374e-02 -6.03812374e-03 ... -8.40878394e-03\n",
      "        -6.96678355e-04 -6.40617125e-03]\n",
      "       [ 1.22143803e-02 -1.78123396e-02  1.83438102e-03 ... -2.97719322e-04\n",
      "        -9.66396276e-03  4.35212511e-04]\n",
      "       [-7.05106789e-03  7.09587103e-03 -9.89367720e-03 ...  1.96752977e-03\n",
      "        -5.22061950e-03  5.43989067e-04]]]\n",
      "    \n",
      "    \n",
      "     [[[-1.06568225e-02  7.72133935e-03 -6.75825810e-04 ... -4.41153394e-03\n",
      "         1.55264055e-02 -2.31100857e-04]\n",
      "       [ 1.71804929e-03 -4.27311286e-03  1.80002153e-02 ... -7.64287484e-04\n",
      "         6.38015394e-04  9.64568928e-03]\n",
      "       [ 2.15227976e-02 -1.19374674e-02 -1.13613624e-03 ... -9.31944698e-03\n",
      "        -4.22954513e-03 -3.07917758e-03]\n",
      "       [-2.88647832e-03 -1.43616470e-02 -4.29879967e-03 ... -6.96344301e-03\n",
      "        -4.50490089e-03  1.32650777e-03]]]], bias_init=<mindspore.common.initializer.Uniform object at 0x766ff6fcc610>, format=NCHW>\n",
      "    (1): ReLU<>\n",
      "    (2): MaxPool2d<kernel_size=(48, 1), stride=1, pad_mode=VALID>\n",
      "    >\n",
      "  (layer3): SequentialCell<\n",
      "    (0): Conv2d<input_channels=1, output_channels=96, kernel_size=(5, 40), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=[[[[-1.25464778e-02 -1.51451500e-02  5.15140034e-03 ... -1.65051520e-02\n",
      "         1.05671976e-02 -7.05008907e-03]\n",
      "       [-2.35896651e-02 -1.18113142e-02  4.96609777e-04 ... -4.41775564e-03\n",
      "        -4.99842595e-03  4.15162928e-03]\n",
      "       [-8.98746122e-03 -7.47644948e-03  1.39575526e-02 ...  1.43439751e-02\n",
      "        -4.74810804e-04  1.34456838e-02]\n",
      "       [-6.42689038e-03 -1.62929168e-03  5.92067838e-03 ...  3.91449500e-03\n",
      "         3.82666360e-03  4.93316038e-04]\n",
      "       [ 3.27619462e-04 -8.04155692e-03 -6.35275105e-03 ... -1.97166367e-03\n",
      "         4.43164119e-03  1.29806483e-02]]]\n",
      "    \n",
      "    \n",
      "     [[[ 1.35923075e-02 -1.36728520e-02 -8.24524928e-03 ...  8.24724883e-03\n",
      "        -3.81074334e-03 -1.13629522e-02]\n",
      "       [-1.12543572e-02 -7.35031767e-03  2.41604284e-03 ...  1.61490694e-03\n",
      "         2.54801661e-03  1.62072312e-02]\n",
      "       [ 1.29262137e-03 -8.85198731e-03  7.33294850e-03 ...  2.78260937e-04\n",
      "        -3.51338740e-03 -3.61082330e-03]\n",
      "       [-1.38186999e-02 -1.96219771e-03  1.95981003e-02 ... -7.54035264e-03\n",
      "        -9.69202549e-04  1.54094875e-03]\n",
      "       [-3.63804190e-03 -2.84346845e-03 -1.32412848e-03 ...  7.48478575e-03\n",
      "         8.84666946e-03  3.14148841e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[-2.03241222e-02  7.01696100e-03  6.20469451e-03 ...  1.06323585e-02\n",
      "        -1.34762023e-02  1.00219680e-03]\n",
      "       [ 5.21168858e-03  1.03328819e-03 -5.77564468e-04 ...  1.10866483e-02\n",
      "         1.30512211e-02  7.19476677e-03]\n",
      "       [ 1.46343578e-02 -1.12121832e-02 -3.15798170e-05 ... -8.24341644e-03\n",
      "        -4.16212296e-03 -2.33263336e-02]\n",
      "       [ 1.24557968e-02 -8.32781289e-03  1.14093826e-03 ...  1.03457868e-02\n",
      "         1.40296435e-02  1.14212371e-03]\n",
      "       [ 8.19731096e-04 -3.37968487e-03  7.95865618e-03 ...  7.16191856e-03\n",
      "        -5.19998046e-03 -3.75284883e-03]]]\n",
      "    \n",
      "    \n",
      "     ...\n",
      "    \n",
      "    \n",
      "     [[[-3.54042603e-03  9.20883380e-03  6.23318413e-03 ... -6.94524590e-03\n",
      "         2.93213688e-03 -8.70767608e-03]\n",
      "       [ 3.94364959e-03 -2.58325809e-03  4.44654888e-03 ...  2.26262957e-04\n",
      "         4.69284831e-03  1.19836035e-03]\n",
      "       [ 8.77243653e-03 -3.53751960e-03 -1.15848249e-02 ...  1.04935085e-02\n",
      "        -1.57272890e-02 -9.71266255e-03]\n",
      "       [ 4.02210222e-04 -1.53668998e-02 -1.35211824e-02 ...  2.28254870e-03\n",
      "        -1.15171522e-02  9.87709966e-03]\n",
      "       [-1.52374934e-02 -9.53715947e-03 -1.31093822e-02 ...  1.31842047e-02\n",
      "         9.60830785e-03 -5.45956194e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[-1.05772931e-02 -5.38871936e-05  6.89081941e-03 ... -1.38859451e-02\n",
      "        -8.18334054e-03  1.44481775e-03]\n",
      "       [ 6.92147203e-03  2.86859367e-03 -8.29998590e-03 ...  2.20237090e-03\n",
      "        -6.90391008e-03  5.55354403e-03]\n",
      "       [-3.95000726e-03 -2.29993113e-03  1.05328895e-02 ... -8.65274295e-03\n",
      "        -2.84572095e-02 -1.17263943e-02]\n",
      "       [ 1.22411246e-03 -2.30626974e-05 -1.79221593e-02 ...  2.13593151e-02\n",
      "        -1.42267672e-02 -2.74534933e-02]\n",
      "       [ 1.82439983e-02 -1.01774735e-02  1.84946251e-03 ... -1.99242141e-02\n",
      "        -5.90227731e-03 -6.31699013e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[ 9.93653107e-03 -4.60160710e-03  2.47676182e-03 ... -1.91338006e-02\n",
      "        -1.79584965e-03 -1.31353154e-03]\n",
      "       [ 3.13386694e-03  6.94298139e-03  8.69465247e-03 ... -3.59103642e-03\n",
      "         1.97366979e-02 -5.06820111e-03]\n",
      "       [-1.40154157e-02  1.56292766e-02 -2.99755717e-03 ... -5.82622085e-03\n",
      "        -2.26922589e-03  3.03173647e-03]\n",
      "       [-3.57237505e-03 -3.55588482e-03 -1.58728510e-02 ...  1.14040393e-02\n",
      "         5.96039183e-03  2.00131871e-02]\n",
      "       [ 4.16106079e-03 -1.40345162e-02 -3.77122592e-03 ...  2.08712630e-02\n",
      "         8.49830825e-03  1.20550185e-03]]]], bias_init=<mindspore.common.initializer.Uniform object at 0x766ff6fcce10>, format=NCHW>\n",
      "    (1): ReLU<>\n",
      "    (2): MaxPool2d<kernel_size=(47, 1), stride=1, pad_mode=VALID>\n",
      "    >\n",
      "  (fc): Dense<input_channels=288, output_channels=2, has_bias=True>\n",
      "  (drop): Dropout<keep_prob=0.5>\n",
      "  >\n"
     ]
    }
   ],
   "source": [
    "# Create TextCNN model instance with vocabulary and configuration parameters\n",
    "net = TextCNN(vocab_len=instance.get_dict_len(), \n",
    "              word_len=cfg.word_len, \n",
    "              num_classes=cfg.num_classes, \n",
    "              vec_length=cfg.vec_length)\n",
    "\n",
    "print(\"TextCNN Model Architecture:\")\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e64d0a6",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Loss Function and Optimizer Configuration\n",
    "Setting up the training components for TextCNN optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff24f670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure optimizer and loss function\n",
    "opt = nn.Adam(params=net.trainable_params(), learning_rate=learning_rate, weight_decay=cfg.weight_decay)\n",
    "loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n",
    "\n",
    "# Create model with optimizer, loss, and metrics\n",
    "model = Model(net, loss_fn=loss, optimizer=opt, metrics={'acc': Accuracy()})\n",
    "\n",
    "# Configure checkpoint settings\n",
    "config_ck = CheckpointConfig(save_checkpoint_steps=batch_num, keep_checkpoint_max=cfg.keep_checkpoint_max)\n",
    "ckpt_save_dir = \"./ckpt\"\n",
    "\n",
    "# Setup training callbacks\n",
    "time_cb = TimeMonitor(data_size=batch_num)\n",
    "ckpoint_cb = ModelCheckpoint(prefix=\"train_textcnn\", directory=ckpt_save_dir, config=config_ck)\n",
    "loss_cb = LossMonitor(per_print_times=batch_num)  # Print loss at the end of each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8193a31d",
   "metadata": {},
   "source": [
    "## üìà Model Training\n",
    "Training the TextCNN network on the sentiment analysis dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fc89763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(11479:130224391405696,MainProcess):2025-09-12-15:52:38.148.257 [mindspore/nn/layer/basic.py:199] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting TextCNN training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(11479:130224391405696,MainProcess):2025-09-12-15:52:38.161.944 [mindspore/nn/layer/basic.py:199] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(11479:130224391405696,MainProcess):2025-09-12-15:52:38.170.829 [mindspore/nn/layer/basic.py:199] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(11479:130224391405696,MainProcess):2025-09-12-15:52:38.170.829 [mindspore/nn/layer/basic.py:199] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(11479:130224391405696,MainProcess):2025-09-12-15:52:38.195.468 [mindspore/nn/layer/basic.py:199] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(11479:130224391405696,MainProcess):2025-09-12-15:52:38.200.064 [mindspore/nn/layer/basic.py:199] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(11479:130224391405696,MainProcess):2025-09-12-15:52:38.195.468 [mindspore/nn/layer/basic.py:199] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(11479:130224391405696,MainProcess):2025-09-12-15:52:38.200.064 [mindspore/nn/layer/basic.py:199] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(11479:130224391405696,MainProcess):2025-09-12-15:52:38.209.339 [mindspore/nn/layer/basic.py:199] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(11479:130224391405696,MainProcess):2025-09-12-15:52:38.209.339 [mindspore/nn/layer/basic.py:199] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[ERROR] CORE(11479,76703a974080,python):2025-09-12-15:52:38.528.636 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11479/3869061871.py]\n",
      "[ERROR] CORE(11479,76703a974080,python):2025-09-12-15:52:38.529.925 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11479/3869061871.py]\n",
      "[ERROR] CORE(11479,76703a974080,python):2025-09-12-15:52:38.529.974 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11479/3869061871.py]\n",
      "[ERROR] CORE(11479,76703a974080,python):2025-09-12-15:52:38.529.984 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11479/3869061871.py]\n",
      "[ERROR] CORE(11479,76703a974080,python):2025-09-12-15:52:38.530.006 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11479/3869061871.py]\n",
      "[ERROR] CORE(11479,76703a974080,python):2025-09-12-15:52:38.530.018 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11479/3869061871.py]\n",
      "[ERROR] CORE(11479,76703a974080,python):2025-09-12-15:52:38.530.051 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11479/3869061871.py]\n",
      "[ERROR] CORE(11479,76703a974080,python):2025-09-12-15:52:38.530.056 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11479/3869061871.py]\n",
      "[ERROR] CORE(11479,76703a974080,python):2025-09-12-15:52:38.530.067 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11479/3869061871.py]\n",
      "[ERROR] CORE(11479,76703a974080,python):2025-09-12-15:52:38.530.072 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11479/3869061871.py]\n",
      "[ERROR] CORE(11479,76703a974080,python):2025-09-12-15:52:38.530.081 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11479/3869061871.py]\n",
      "[ERROR] CORE(11479,76703a974080,python):2025-09-12-15:52:38.530.086 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11479/3869061871.py]\n",
      "[ERROR] CORE(11479,76703a974080,python):2025-09-12-15:52:38.528.636 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11479/3869061871.py]\n",
      "[ERROR] CORE(11479,76703a974080,python):2025-09-12-15:52:38.529.925 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11479/3869061871.py]\n",
      "[ERROR] CORE(11479,76703a974080,python):2025-09-12-15:52:38.529.974 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11479/3869061871.py]\n",
      "[ERROR] CORE(11479,76703a974080,python):2025-09-12-15:52:38.529.984 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11479/3869061871.py]\n",
      "[ERROR] CORE(11479,76703a974080,python):2025-09-12-15:52:38.530.006 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11479/3869061871.py]\n",
      "[ERROR] CORE(11479,76703a974080,python):2025-09-12-15:52:38.530.018 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11479/3869061871.py]\n",
      "[ERROR] CORE(11479,76703a974080,python):2025-09-12-15:52:38.530.051 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11479/3869061871.py]\n",
      "[ERROR] CORE(11479,76703a974080,python):2025-09-12-15:52:38.530.056 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11479/3869061871.py]\n",
      "[ERROR] CORE(11479,76703a974080,python):2025-09-12-15:52:38.530.067 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11479/3869061871.py]\n",
      "[ERROR] CORE(11479,76703a974080,python):2025-09-12-15:52:38.530.072 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11479/3869061871.py]\n",
      "[ERROR] CORE(11479,76703a974080,python):2025-09-12-15:52:38.530.081 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11479/3869061871.py]\n",
      "[ERROR] CORE(11479,76703a974080,python):2025-09-12-15:52:38.530.086 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11479/3869061871.py]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 596, loss is 0.05056581273674965\n",
      "Train epoch time: 21509.459 ms, per step time: 36.090 ms\n",
      "Train epoch time: 21509.459 ms, per step time: 36.090 ms\n",
      "epoch: 2 step: 596, loss is 0.0029518890660256147\n",
      "Train epoch time: 20455.461 ms, per step time: 34.321 ms\n",
      "epoch: 2 step: 596, loss is 0.0029518890660256147\n",
      "Train epoch time: 20455.461 ms, per step time: 34.321 ms\n",
      "epoch: 3 step: 596, loss is 0.002310193609446287\n",
      "Train epoch time: 18841.412 ms, per step time: 31.613 ms\n",
      "epoch: 3 step: 596, loss is 0.002310193609446287\n",
      "Train epoch time: 18841.412 ms, per step time: 31.613 ms\n",
      "epoch: 4 step: 596, loss is 0.002011424396187067\n",
      "Train epoch time: 18965.490 ms, per step time: 31.821 ms\n",
      "Training completed successfully!\n",
      "epoch: 4 step: 596, loss is 0.002011424396187067\n",
      "Train epoch time: 18965.490 ms, per step time: 31.821 ms\n",
      "Training completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Start model training with configured callbacks\n",
    "print(\"Starting TextCNN training...\")\n",
    "model.train(cfg.epoch_size, dataset, callbacks=[time_cb, ckpoint_cb, loss_cb])\n",
    "print(\"Training completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de57e3c8",
   "metadata": {},
   "source": [
    "## üéØ Model Validation and Testing\n",
    "Validating the trained TextCNN model on the sentiment analysis test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c303ffe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(11479:130224391405696,MainProcess):2025-09-12-15:53:57.963.223 [mindspore/nn/layer/basic.py:199] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(11479:130224391405696,MainProcess):2025-09-12-15:53:57.970.669 [mindspore/nn/layer/basic.py:199] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(11479:130224391405696,MainProcess):2025-09-12-15:53:57.970.669 [mindspore/nn/layer/basic.py:199] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(11479:130224391405696,MainProcess):2025-09-12-15:53:57.975.811 [mindspore/nn/layer/basic.py:199] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(11479:130224391405696,MainProcess):2025-09-12-15:53:57.975.811 [mindspore/nn/layer/basic.py:199] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Evaluating model on test dataset...\n",
      "Test accuracy: 0.7588\n",
      "Test accuracy: 0.7588\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model from checkpoint\n",
    "param_dict = load_checkpoint(cfg.checkpoint_path)\n",
    "load_param_into_net(net, param_dict)\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Create test dataset for evaluation\n",
    "test_dataset = instance.create_test_dataset(batch_size=cfg.batch_size)\n",
    "\n",
    "# Evaluate the model on test set\n",
    "print(\"Evaluating model on test dataset...\")\n",
    "result = model.eval(test_dataset, dataset_sink_mode=False)\n",
    "print(\"Test accuracy: {:.4f}\".format(result['acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4ccdf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing function for inference\n",
    "def preprocess(sentence):\n",
    "    \"\"\"\n",
    "    Preprocess input text for sentiment analysis inference.\n",
    "    \n",
    "    Args:\n",
    "        sentence (str): Raw input text to be processed\n",
    "        \n",
    "    Returns:\n",
    "        list: Vectorized representation of the input text\n",
    "    \"\"\"\n",
    "    # Convert to lowercase and strip whitespace\n",
    "    sentence = sentence.lower().strip()\n",
    "    \n",
    "    # Remove punctuation and special characters (same as training preprocessing)\n",
    "    sentence = sentence.replace('\\n','')\\\n",
    "        .replace('\"','')\\\n",
    "        .replace('\\'','')\\\n",
    "        .replace('.','')\\\n",
    "        .replace(',','')\\\n",
    "        .replace('[','')\\\n",
    "        .replace(']','')\\\n",
    "        .replace('(','')\\\n",
    "        .replace(')','')\\\n",
    "        .replace(':','')\\\n",
    "        .replace('--','')\\\n",
    "        .replace('-',' ')\\\n",
    "        .replace('\\\\','')\\\n",
    "        .replace('0','')\\\n",
    "        .replace('1','')\\\n",
    "        .replace('2','')\\\n",
    "        .replace('3','')\\\n",
    "        .replace('4','')\\\n",
    "        .replace('5','')\\\n",
    "        .replace('6','')\\\n",
    "        .replace('7','')\\\n",
    "        .replace('8','')\\\n",
    "        .replace('9','')\\\n",
    "        .replace('`','')\\\n",
    "        .replace('=','')\\\n",
    "        .replace('$','')\\\n",
    "        .replace('/','')\\\n",
    "        .replace('*','')\\\n",
    "        .replace(';','')\\\n",
    "        .replace('<b>','')\\\n",
    "        .replace('%','')\\\n",
    "        .replace(\"  \",\" \")  # Replace double spaces with single space\n",
    "    \n",
    "    # Tokenize the sentence\n",
    "    sentence = sentence.split(' ')\n",
    "    \n",
    "    # Convert tokens to vector representation\n",
    "    maxlen = cfg.word_len\n",
    "    vector = [0] * maxlen\n",
    "    \n",
    "    for index, word in enumerate(sentence):\n",
    "        if index >= maxlen:\n",
    "            break\n",
    "        if word not in instance.Vocab.keys():\n",
    "            print(f\"'{word}' - The word does not appear in the dictionary.\")\n",
    "        else:\n",
    "            vector[index] = instance.Vocab[word]\n",
    "    \n",
    "    return vector\n",
    "\n",
    "\n",
    "# Inference function for sentiment prediction\n",
    "def inference(review_en):\n",
    "    \"\"\"\n",
    "    Perform sentiment analysis inference on input text.\n",
    "    \n",
    "    Args:\n",
    "        review_en (str): Input review text in English\n",
    "        \n",
    "    Returns:\n",
    "        None: Prints the sentiment prediction result\n",
    "    \"\"\"\n",
    "    # Preprocess the input text\n",
    "    review_en = preprocess(review_en)\n",
    "    \n",
    "    # Convert to tensor format for model input\n",
    "    input_en = Tensor(np.array([review_en]).astype(np.int32))\n",
    "    \n",
    "    # Get model prediction\n",
    "    output = net(input_en)\n",
    "    \n",
    "    # Interpret the prediction result\n",
    "    if np.argmax(np.array(output[0])) == 1:\n",
    "        print(\"Positive comments\")\n",
    "    else:\n",
    "        print(\"Negative comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b824e95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(11479:130224391405696,MainProcess):2025-09-12-15:53:58.227.927 [mindspore/nn/layer/basic.py:199] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(11479:130224391405696,MainProcess):2025-09-12-15:53:58.235.686 [mindspore/nn/layer/basic.py:199] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(11479:130224391405696,MainProcess):2025-09-12-15:53:58.235.686 [mindspore/nn/layer/basic.py:199] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input review: 'the movie is so boring'\n",
      "Prediction:\n",
      "Negative comments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] CORE(11479,76703a974080,python):2025-09-12-15:53:58.278.697 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11479/3869061871.py]\n",
      "[ERROR] CORE(11479,76703a974080,python):2025-09-12-15:53:58.278.740 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11479/3869061871.py]\n",
      "[ERROR] CORE(11479,76703a974080,python):2025-09-12-15:53:58.278.770 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_11479/3869061871.py]\n"
     ]
    }
   ],
   "source": [
    "# Test the model with a sample review\n",
    "review_en = \"the movie is so boring\"\n",
    "print(f\"Input review: '{review_en}'\")\n",
    "print(\"Prediction:\")\n",
    "inference(review_en)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
